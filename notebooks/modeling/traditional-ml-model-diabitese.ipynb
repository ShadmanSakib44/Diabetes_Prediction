{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10750009,"sourceType":"datasetVersion","datasetId":6667203},{"sourceId":11731946,"sourceType":"datasetVersion","datasetId":7364801},{"sourceId":27685252,"sourceType":"kernelVersion"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('/kaggle/input/diabetes/diabetes_input.csv')\n\n# Display first 5 rows\nprint(\"First 5 rows of the dataset:\")\nprint(df.head())\n\n# Dataset Info\nprint(\"\\nDataset Info:\")\nprint(df.info())\n\n# Summary Statistics\nprint(\"\\nSummary Statistics:\")\nprint(df.describe())\n\n# Missing Values\nprint(\"\\nMissing Values:\")\nprint(df.isnull().sum())\n\n# Check for 'Outcome' column and compute class distribution\nif 'Outcome' in df.columns:\n    print(\"\\nClass Distribution (Outcome):\")\n    print(df['Outcome'].value_counts())\n    print(\"\\nPercentage Distribution:\")\n    print(df['Outcome'].value_counts(normalize=True) * 100)\nelse:\n    print(\"\\n⚠️ 'Outcome' column not found in dataset.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T10:18:52.534111Z","iopub.execute_input":"2025-05-08T10:18:52.534656Z","iopub.status.idle":"2025-05-08T10:18:52.571418Z","shell.execute_reply.started":"2025-05-08T10:18:52.534631Z","shell.execute_reply":"2025-05-08T10:18:52.570853Z"}},"outputs":[{"name":"stdout","text":"First 5 rows of the dataset:\n   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n0            6      148             72             35        0  33.6   \n1            1       85             66             29        0  26.6   \n2            8      183             64              0        0  23.3   \n3            1       89             66             23       94  28.1   \n4            0      137             40             35      168  43.1   \n\n   DiabetesPedigreeFunction  Age  \n0                     0.627   50  \n1                     0.351   31  \n2                     0.672   32  \n3                     0.167   21  \n4                     2.288   33  \n\nDataset Info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 768 entries, 0 to 767\nData columns (total 8 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   Pregnancies               768 non-null    int64  \n 1   Glucose                   768 non-null    int64  \n 2   BloodPressure             768 non-null    int64  \n 3   SkinThickness             768 non-null    int64  \n 4   Insulin                   768 non-null    int64  \n 5   BMI                       768 non-null    float64\n 6   DiabetesPedigreeFunction  768 non-null    float64\n 7   Age                       768 non-null    int64  \ndtypes: float64(2), int64(6)\nmemory usage: 48.1 KB\nNone\n\nSummary Statistics:\n       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\ncount   768.000000  768.000000     768.000000     768.000000  768.000000   \nmean      3.845052  120.894531      69.105469      20.536458   79.799479   \nstd       3.369578   31.972618      19.355807      15.952218  115.244002   \nmin       0.000000    0.000000       0.000000       0.000000    0.000000   \n25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n75%       6.000000  140.250000      80.000000      32.000000  127.250000   \nmax      17.000000  199.000000     122.000000      99.000000  846.000000   \n\n              BMI  DiabetesPedigreeFunction         Age  \ncount  768.000000                768.000000  768.000000  \nmean    31.992578                  0.471876   33.240885  \nstd      7.884160                  0.331329   11.760232  \nmin      0.000000                  0.078000   21.000000  \n25%     27.300000                  0.243750   24.000000  \n50%     32.000000                  0.372500   29.000000  \n75%     36.600000                  0.626250   41.000000  \nmax     67.100000                  2.420000   81.000000  \n\nMissing Values:\nPregnancies                 0\nGlucose                     0\nBloodPressure               0\nSkinThickness               0\nInsulin                     0\nBMI                         0\nDiabetesPedigreeFunction    0\nAge                         0\ndtype: int64\n\n⚠️ 'Outcome' column not found in dataset.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Load the dataset (update the file path as needed)\ndf = pd.read_csv('/kaggle/input/diabetes-with-outcome/diabetes.csv')\n\n# Separate features and target\nX = df.drop(columns=['Outcome'])\ny = df['Outcome']\n\n# Train-test split (80/20)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Feature scaling\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Initialize models\nmodels = {\n    'Logistic Regression': LogisticRegression(),\n    'Random Forest': RandomForestClassifier(),\n    'SVM': SVC()\n}\n\n# Train and evaluate each model\nresults = []\nfor name, model in models.items():\n    model.fit(X_train_scaled, y_train)\n    y_pred = model.predict(X_test_scaled)\n    results.append({\n        'Model': name,\n        'Accuracy': accuracy_score(y_test, y_pred),\n        'Precision': precision_score(y_test, y_pred),\n        'Recall': recall_score(y_test, y_pred),\n        'F1-Score': f1_score(y_test, y_pred)\n    })\n\n# Convert results to DataFrame\nresults_df = pd.DataFrame(results)\nprint(results_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:24:14.834325Z","iopub.execute_input":"2025-05-08T14:24:14.835150Z","iopub.status.idle":"2025-05-08T14:24:15.170113Z","shell.execute_reply.started":"2025-05-08T14:24:14.835121Z","shell.execute_reply":"2025-05-08T14:24:15.169343Z"}},"outputs":[{"name":"stdout","text":"                 Model  Accuracy  Precision  Recall  F1-Score\n0  Logistic Regression  0.735931   0.617284  0.6250  0.621118\n1        Random Forest  0.740260   0.621951  0.6375  0.629630\n2                  SVM  0.744589   0.643836  0.5875  0.614379\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n\n# Load the dataset\ndf = pd.read_csv('/kaggle/input/diabetes-with-outcome/diabetes.csv')\n\n# Separate features and target\nX = df.drop(columns=['Outcome'])\ny = df['Outcome']\n\n# Feature scaling\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Initialize models\nmodels = {\n    'Logistic Regression': LogisticRegression(),\n    'Random Forest': RandomForestClassifier(),\n    'SVM': SVC()\n}\n\n# Scoring metrics\nscoring = {\n    'accuracy': 'accuracy',\n    'precision': 'precision',\n    'recall': 'recall',\n    'f1': 'f1'\n}\n\n# Cross-validation\nresults = []\nfor name, model in models.items():\n    scores = cross_validate(model, X_scaled, y, cv=10, scoring=scoring)\n    results.append({\n        'Model': name,\n        'Accuracy': scores['test_accuracy'].mean(),\n        'Precision': scores['test_precision'].mean(),\n        'Recall': scores['test_recall'].mean(),\n        'F1-Score': scores['test_f1'].mean()\n    })\n\n# Convert results to DataFrame\nresults_df = pd.DataFrame(results)\nprint(results_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T13:06:05.382599Z","iopub.execute_input":"2025-05-08T13:06:05.382832Z","iopub.status.idle":"2025-05-08T13:06:09.590444Z","shell.execute_reply.started":"2025-05-08T13:06:05.382814Z","shell.execute_reply":"2025-05-08T13:06:09.589809Z"}},"outputs":[{"name":"stdout","text":"                 Model  Accuracy  Precision    Recall  F1-Score\n0  Logistic Regression  0.772180   0.731493  0.559687  0.631430\n1        Random Forest  0.768182   0.702923  0.596866  0.640942\n2                  SVM  0.760390   0.707082  0.548148  0.613324\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n# Import all required classifiers\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.tree import DecisionTreeClassifier as DTC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import Perceptron, LogisticRegression\nfrom sklearn.tree import ExtraTreeClassifier\n\n# Load the dataset\ndf = pd.read_csv('/kaggle/input/diabetes-with-outcome/diabetes.csv')\nX = df.drop(columns=['Outcome'])\ny = df['Outcome']\n\n# Feature scaling\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n\n# List of models\nmn = [\n    KNN(), DTC(), GaussianNB(), LDA(), SVC(), LinearSVC(max_iter=10000),\n    AdaBoostClassifier(), RandomForestClassifier(), Perceptron(),\n    ExtraTreeClassifier(), BaggingClassifier(), LogisticRegression(max_iter=10000),\n    GradientBoostingClassifier()\n]\n\nmodel_names = [\n    \"KNN\", \"Decision Tree\", \"GaussianNB\", \"LDA\", \"SVC\", \"LinearSVC\",\n    \"AdaBoost\", \"Random Forest\", \"Perceptron\", \"Extra Tree\", \"Bagging\",\n    \"Logistic Regression\", \"Gradient Boosting\"\n]\n\n# Train, predict, and evaluate\nfor i in range(len(mn)):\n    print(f\"\\nModel {i+1}: {model_names[i]}\")\n    model = mn[i]\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:24:26.351849Z","iopub.execute_input":"2025-05-08T14:24:26.352558Z","iopub.status.idle":"2025-05-08T14:24:27.057010Z","shell.execute_reply.started":"2025-05-08T14:24:26.352536Z","shell.execute_reply":"2025-05-08T14:24:27.056275Z"}},"outputs":[{"name":"stdout","text":"\nModel 1: KNN\nAccuracy: 0.6926406926406926\nConfusion Matrix:\n [[119  32]\n [ 39  41]]\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.75      0.79      0.77       151\n           1       0.56      0.51      0.54        80\n\n    accuracy                           0.69       231\n   macro avg       0.66      0.65      0.65       231\nweighted avg       0.69      0.69      0.69       231\n\n\nModel 2: Decision Tree\nAccuracy: 0.70995670995671\nConfusion Matrix:\n [[110  41]\n [ 26  54]]\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.81      0.73      0.77       151\n           1       0.57      0.68      0.62        80\n\n    accuracy                           0.71       231\n   macro avg       0.69      0.70      0.69       231\nweighted avg       0.73      0.71      0.71       231\n\n\nModel 3: GaussianNB\nAccuracy: 0.7445887445887446\nConfusion Matrix:\n [[119  32]\n [ 27  53]]\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.82      0.79      0.80       151\n           1       0.62      0.66      0.64        80\n\n    accuracy                           0.74       231\n   macro avg       0.72      0.73      0.72       231\nweighted avg       0.75      0.74      0.75       231\n\n\nModel 4: LDA\nAccuracy: 0.7316017316017316\nConfusion Matrix:\n [[120  31]\n [ 31  49]]\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.79      0.79      0.79       151\n           1       0.61      0.61      0.61        80\n\n    accuracy                           0.73       231\n   macro avg       0.70      0.70      0.70       231\nweighted avg       0.73      0.73      0.73       231\n\n\nModel 5: SVC\nAccuracy: 0.7489177489177489\nConfusion Matrix:\n [[125  26]\n [ 32  48]]\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.80      0.83      0.81       151\n           1       0.65      0.60      0.62        80\n\n    accuracy                           0.75       231\n   macro avg       0.72      0.71      0.72       231\nweighted avg       0.75      0.75      0.75       231\n\n\nModel 6: LinearSVC\nAccuracy: 0.7359307359307359\nConfusion Matrix:\n [[120  31]\n [ 30  50]]\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.80      0.79      0.80       151\n           1       0.62      0.62      0.62        80\n\n    accuracy                           0.74       231\n   macro avg       0.71      0.71      0.71       231\nweighted avg       0.74      0.74      0.74       231\n\n\nModel 7: AdaBoost\nAccuracy: 0.7402597402597403\nConfusion Matrix:\n [[121  30]\n [ 30  50]]\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.80      0.80      0.80       151\n           1       0.62      0.62      0.62        80\n\n    accuracy                           0.74       231\n   macro avg       0.71      0.71      0.71       231\nweighted avg       0.74      0.74      0.74       231\n\n\nModel 8: Random Forest\nAccuracy: 0.7489177489177489\nConfusion Matrix:\n [[124  27]\n [ 31  49]]\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.80      0.82      0.81       151\n           1       0.64      0.61      0.63        80\n\n    accuracy                           0.75       231\n   macro avg       0.72      0.72      0.72       231\nweighted avg       0.75      0.75      0.75       231\n\n\nModel 9: Perceptron\nAccuracy: 0.645021645021645\nConfusion Matrix:\n [[92 59]\n [23 57]]\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.80      0.61      0.69       151\n           1       0.49      0.71      0.58        80\n\n    accuracy                           0.65       231\n   macro avg       0.65      0.66      0.64       231\nweighted avg       0.69      0.65      0.65       231\n\n\nModel 10: Extra Tree\nAccuracy: 0.6623376623376623\nConfusion Matrix:\n [[106  45]\n [ 33  47]]\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.76      0.70      0.73       151\n           1       0.51      0.59      0.55        80\n\n    accuracy                           0.66       231\n   macro avg       0.64      0.64      0.64       231\nweighted avg       0.68      0.66      0.67       231\n\n\nModel 11: Bagging\nAccuracy: 0.7316017316017316\nConfusion Matrix:\n [[121  30]\n [ 32  48]]\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.79      0.80      0.80       151\n           1       0.62      0.60      0.61        80\n\n    accuracy                           0.73       231\n   macro avg       0.70      0.70      0.70       231\nweighted avg       0.73      0.73      0.73       231\n\n\nModel 12: Logistic Regression\nAccuracy: 0.7359307359307359\nConfusion Matrix:\n [[120  31]\n [ 30  50]]\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.80      0.79      0.80       151\n           1       0.62      0.62      0.62        80\n\n    accuracy                           0.74       231\n   macro avg       0.71      0.71      0.71       231\nweighted avg       0.74      0.74      0.74       231\n\n\nModel 13: Gradient Boosting\nAccuracy: 0.7532467532467533\nConfusion Matrix:\n [[121  30]\n [ 27  53]]\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.82      0.80      0.81       151\n           1       0.64      0.66      0.65        80\n\n    accuracy                           0.75       231\n   macro avg       0.73      0.73      0.73       231\nweighted avg       0.76      0.75      0.75       231\n\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}